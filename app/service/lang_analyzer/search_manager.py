# search_manager.py

import re
from pathlib import Path

from PyKomoran import Komoran
from whoosh.analysis import Tokenizer, LowercaseFilter, Token
from whoosh.fields import Schema, ID, TEXT
from whoosh.index import open_dir, create_in, exists_in

from app.service.lang_analyzer.synonym_filter import CustomSynonymFilter

# ë™ì˜ì–´ ì‚¬ì „: ë°˜ë“œì‹œ ìˆœìˆ˜ dict/list í˜•íƒœë¡œ ê´€ë¦¬ (dict_keys ì‚¬ìš© ê¸ˆì§€)
my_synonyms = {
    "íœ´ëŒ€í°": ["ìŠ¤ë§ˆíŠ¸í°", "í•¸ë“œí°"],
    "ë…¸íŠ¸": ["ë¬¸ì„œ", "ê¸°ë¡"],
    "fastapi": ["íŒŒìŠ¤íŠ¸api", "ë°±ì—”ë“œ"]
}

# ğŸ’¡ 1. Komoran ê°ì²´ë¥¼ ì „ì—­(Global) ì˜ì—­ì—ì„œ ì´ˆê¸°í™”
# ì´ë ‡ê²Œ í•˜ë©´ KoEnTokenizer ì¸ìŠ¤í„´ìŠ¤ ë‚´ë¶€ì— í¬í•¨ë˜ì§€ ì•Šì•„ pickle ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
_KOMORAN_INSTANCE = Komoran("EXP")


class KoEnTokenizer(Tokenizer):
    """ í•œê¸€/ì˜ì–´ ë³µí•© ëª…ì‚¬ ë¶„í•´ ë° ì¡°ì‚¬ ì œê±° í† í¬ë‚˜ì´ì € """

    def __init__(self):
        # ëª…ì‚¬ ì¶”ì¶œì„ ìœ„í•œ Komoran ê°ì²´ ìƒì„±
        # ì˜ì–´ ë° ìˆ«ìë¥¼ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•œ ì •ê·œí‘œí˜„ì‹ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
        self.en_pattern = re.compile(r'[a-zA-Z0-9]+')

    def __call__(self, value, positions=False, chars=False, **kwargs):
        """
        :param value: ì¸ë±ì‹±í•  ì›ë¬¸ í…ìŠ¤íŠ¸ (ì˜ˆ: "FastAPIë¥¼ ì´ìš©í•œ Note í”„ë¡œì íŠ¸")
        :param positions:
        :param chars:
        :param kwargs:
        :return:
        """
        # 1. í•œê¸€ í˜•íƒ¯ ë¶„ì„ (ë³µí•© ëª…ì‚¬ ë¶„í•´ í¬í•¨)
        # get_plain_textëŠ” 'ë‹¨ì–´/í’ˆì‚¬' í˜•íƒœë¡œ ë°˜í™˜í•˜ë¯ˆ ëª…ì‚¬ (NNG, NNP)ë§Œ ì¶”ì¶œ
        # get_nounsëŠ” [FastAPI, ì´ìš©, Note, í”„ë¡œì íŠ¸] ê°™ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë ¤ í•˜ì§€ë§Œ
        # ì˜ì–´ëŠ” ë¶„ì„ê¸°ì— ë”°ë¼ ëˆ„ë½ë  ìˆ˜ ìˆìœ¼ë¥´ëª¨ ëª…ì‹œì  ì²˜ë¦¬ê°€ ì¢‹ìŒ
        nouns = _KOMORAN_INSTANCE.get_nouns(value)

        # 2. ì˜ì–´ ë° ìˆ«ì ì¶”ì¶œ
        en_words = self.en_pattern.findall(value)

        # 3. ì¤‘ë³µ ì œê±° ë° í† í° ìƒì„±
        # í•œê¸€ ëª…ì‚¬ì™€ ì˜ì–´ ë‹¨ì–´ë¥¼ í•©ì¹œ ë’¤ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
        all_keywords = set(nouns + [w.lower() for w in en_words])

        for i, word in enumerate(all_keywords):
            # Whooshê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” Token ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì–‘ë³´(yield)í•©ë‹ˆë‹¤.
            t = Token(positions, chars, pos=i)
            t.text = word
            yield t


class NoteSearchManager:
    def __init__(self, index_dir="data/index"):
        # 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê³„ì‚°
        self.base_dir = Path(__file__).resolve().parent.parent.parent.parent
        # 2. ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ ì €ì¥
        self.index_path = self.base_dir / index_dir
        analyzer = KoEnTokenizer() | LowercaseFilter() | CustomSynonymFilter(my_synonyms)

        if not self.index_path.exists():
            self.index_path.mkdir(parents=True, exist_ok=True)

        self.schema = Schema(
            title=ID(stored=True, unique=True),  # íŒŒì¼ ì œëª© (ê³ ìœ  ì‹ë³„ì)
            content=TEXT(stored=True, analyzer=analyzer),
        )

        # ğŸ’¡ í´ë”ëŠ” ìˆì§€ë§Œ ìœ íš¨í•œ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ í™•ì‹¤íˆ ì²´í¬
        if not exists_in(str(self.index_path)):
            print(f"ğŸ” [System] ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì¤‘: {self.index_path}")
            create_in(str(self.index_path), self.schema)

        # 4. ì¸ë±ìŠ¤ ì—´ê¸° (WhooshëŠ” ë¬¸ìì—´ ê²½ë¡œë¥¼ ë°›ìœ¼ë¯€ë¡œ str ë³€í™˜)
        self.ix = open_dir(str(self.index_path))

    def update_index(self, title, content):
        """ íŒŒì¼ ì €ì¥/ìˆ˜ì • ì‹œ í˜¸ì¶œ: ê²€ìƒ‰ ì§€ë„ë¥¼ ê°±ì‹  í•©ë‹ˆë‹¤. """
        writer = self.ix.writer()
        writer.update_document(title=title, content=content)
        writer.commit()
        print(f"index {title} updated")

    def delete_index(self, title):
        """ íŒŒì¼ ì‚­ì œ ì‹œ í˜¸ì¶œ: ê²€ìƒ‰ ì§€ë„ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤. """
        writer = self.ix.writer()
        writer.delete_by_term("title", title)
        writer.commit()
        print(f"index {title} deleted")

    def search(self, keyword, limit=10):
        """ ë³¸ë¬¸ ê²€ìƒ‰: í‚¤ì›Œë“œê°€ í¬í•¨ëœ íŒŒì¼ ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ í•©ë‹ˆë‹¤. """
        from whoosh.qparser import QueryParser

        with self.ix.searcher() as searcher:
            parser = QueryParser("content", self.ix.schema)
            query = parser.parse(keyword)
            results = searcher.search(query, limit=limit)
            return [hit['title'] for hit in results]
